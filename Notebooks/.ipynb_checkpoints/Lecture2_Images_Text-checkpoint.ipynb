{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970c1ff-2bc4-4752-a354-a08751f0a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8253b1-0d39-48fa-8f22-caed6f28d89b",
   "metadata": {},
   "source": [
    "Image Data from [here](https://www.kaggle.com/andrewmvd/animal-faces)\n",
    "- Animal Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6849d1a-1445-4015-8d8d-58693590b16f",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704eb37-05a7-47f7-96d0-c8a0d63850c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's in this dataset?\n",
    "os.listdir('course_data/afhq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8d255-f9fd-4564-ba57-0b33120afc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three labels\n",
    "os.listdir('course_data/afhq/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801a987-19d2-4d92-b1fe-cb64af62504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within each folder are the images\n",
    "os.listdir('course_data/afhq/train/cat')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7889c22-13b7-4c62-9b89-604e154067ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for our data\n",
    "data_path = 'course_data/afhq'\n",
    "\n",
    "rows = []\n",
    "for dataset in os.listdir(data_path):\n",
    "    for label in os.listdir(data_path + f'/{dataset}'):\n",
    "        for image in os.listdir(data_path + f'/{dataset}' + f'/{label}'):\n",
    "            row = dict()\n",
    "            row['image_file'] = image\n",
    "            row['label'] = label\n",
    "            row['dataset'] = dataset\n",
    "        \n",
    "            # a bit redudant, could build from other data in __getitem__ if wanted\n",
    "            row['image_path'] = data_path + f'/{dataset}' + f'/{label}'\n",
    "            rows.append(row)\n",
    "        \n",
    "df = pd.DataFrame(rows)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ba42d-5b90-45f9-aed4-defc0834d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation data\n",
    "df_train = df[df['dataset'] == 'train'].reset_index(drop=True)\n",
    "df_val = df[df['dataset'] == 'val'].reset_index(drop=True)\n",
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f7585-3cbf-494a-b88f-af37894a2b5d",
   "metadata": {},
   "source": [
    "Before creating a Dataset class, let's think about what we want as our input to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d10e2-9baa-4b61-a968-8158320bff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# pull up an image\n",
    "row = df.iloc[0]\n",
    "image_path = row['image_path']\n",
    "fname = row['image_file']\n",
    "path = image_path+'/'+fname\n",
    "img = cv2.imread(path)\n",
    "\n",
    "# what is an image?\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d0332-3974-4310-8a37-28375738d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 512x512 image with 3 channels\n",
    "print(img.shape)\n",
    "\n",
    "# pixel intensity goes from 0 to 255\n",
    "print(np.max(img), np.min(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67e75a-4d88-40a9-9845-9b7acf18f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the image\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb447e-6d98-4d83-9e7c-3a12c10ada5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why is it weird? cv2 opens in BGR instead of RGB\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b2cc0d-2f0e-4e43-a208-ca1dc696de92",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d746d-533e-47cb-90c4-860e04d4770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a Dataset for our animal faces! \n",
    "class AnimalFacesDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        # label dictionary\n",
    "        self.label_dict = {'cat':0, 'dog':1, 'wild':2}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # get ingredients for retrieving image\n",
    "        image_path = row['image_path']\n",
    "        fname = row['image_file']\n",
    "        path = image_path+'/'+fname\n",
    "        \n",
    "        # read the img\n",
    "        img = cv2.imread(path)\n",
    "        \n",
    "        # convert to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # move color channels to correct spot\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        \n",
    "        # convert to [0,1] scale\n",
    "        img = torch.tensor(img / 255.).float()\n",
    "        \n",
    "        label = torch.tensor(self.label_dict[row['label']])\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73728328-47d5-47ad-bdf9-7751f993e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = AnimalFacesDataset(df_train)\n",
    "dl_train = DataLoader(ds_train, batch_size = 8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be88e7f-adb1-4d83-b5f8-96bf37aee942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure our recipe works!\n",
    "# notice the time...\n",
    "for img, label in tqdm(dl_train):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3e172-2054-4c88-a585-6ab0739bb405",
   "metadata": {},
   "source": [
    "## Text\n",
    "\n",
    "IMDB Movie Review Dataset (cleaned)\n",
    "- Originally from [here](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "- Cleaned into a csv [here](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25557fb-f1ff-4697-ab5c-59c2d564111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('course_data/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c0b68-f863-4268-baba-17c516239284",
   "metadata": {},
   "source": [
    "## Automatic Tokenization with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f33af7-992a-4089-993c-90f5422e27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool for text\n",
    "import spacy\n",
    "\n",
    "# load information about words\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d150bf-b722-4222-85fd-08a764897b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = df.iloc[9]['review']\n",
    "print(some_text)\n",
    "\n",
    "# automatically tokenize the text\n",
    "tokenized_text = nlp(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3840a77-cce1-495f-8984-4f0faa1c6613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's not perfect\n",
    "for token in tokenized_text:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a198be-6a4d-47e2-8247-5709c866e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "\n",
    "token = tokenized_text[idx]\n",
    "\n",
    "# lemmatization\n",
    "print('Lemmatization of', token.text, 'is', token.lemma_)\n",
    "\n",
    "# part of speech tagging\n",
    "print(token.text, 'is a', token.pos_)\n",
    "\n",
    "# is it a stop word?\n",
    "print('The fact that', token.text, 'is a stop word is', token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8393c2-b9a4-4aa1-a5a4-19a38a0a0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence segmentation\n",
    "for sentence in tokenized_text.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc8a5f6-82ef-4963-9a4f-11a3877e1a34",
   "metadata": {},
   "source": [
    "- tons more fancy features!\n",
    "- Let's do a simple pipeline where we ignore non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f9d86-a405-4d67-bf22-63a85ae600a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "a_review = df.iloc[9]['review']\n",
    "\n",
    "# remove those <br />s\n",
    "a_review = a_review.replace('<br />', ' ')\n",
    "print(a_review)\n",
    "\n",
    "# remove non-alphabetic characters\n",
    "a_review = re.sub(\"[^A-Za-z']+\", ' ', a_review)\n",
    "print(a_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0c2e1-175f-4a18-ad6c-02514e7dd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabling some fancy features of spacy for speed\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser'])\n",
    "\n",
    "rows = []\n",
    "for idx in tqdm(range(len(df))):\n",
    "    row = df.iloc[idx].copy()\n",
    "    \n",
    "    # first we remove numeric characters and lowercase everything\n",
    "    cleaned_review = re.sub(\"[^A-Za-z']+\", ' ', row['review'].replace('<br />', ' ')).lower()\n",
    "    \n",
    "    # we let spaCy tokenize and lemmatize the text for us\n",
    "    tokenized_review = nlp(cleaned_review)\n",
    "    cleaned_tokenized = [token.lemma_ for token in tokenized_review if ((not token.is_stop) or (' ' in token.text))]\n",
    "    \n",
    "    if len(cleaned_tokenized) > 1:\n",
    "        row['cleaned'] = ' '.join(cleaned_tokenized)\n",
    "    rows.append(row)\n",
    "df_clean = pd.DataFrame(rows)\n",
    "df_clean.head()\n",
    "df_clean.to_csv('course_data/IMDB_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696808f-051c-4b1b-8dc1-271e0cb104f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('course_data/IMDB_cleaned.csv')\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4def4-b379-4452-bae8-a962de77bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.iloc[9]['review'])\n",
    "print('\\n')\n",
    "df_clean.iloc[9]['cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760296d3-3b90-481a-9650-54a4beef1c4e",
   "metadata": {},
   "source": [
    "## Prepare for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13d617-7844-4441-96c5-5564a3f1c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words, send infrequent to unknown\n",
    "\n",
    "# let's get an idea of word frequency\n",
    "from collections import Counter\n",
    "\n",
    "reviews = [review.split(' ') for review in list(df_clean['cleaned'])]\n",
    "word_freq = Counter([token for review in reviews for token in review]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc393d-ca0d-4d73-be3b-7e289ea4e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no surprises here\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af9f53-8dfe-4643-bc04-ee055a4d9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words only seen once\n",
    "word_freq[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec74f1-3877-4288-bab7-8807a2a40f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that appear infrequently\n",
    "word_freq = dict(word_freq)\n",
    "print(len(word_freq))\n",
    "min_freq = 5\n",
    "word_dict = {}\n",
    "\n",
    "# sending all the unknowns to 0\n",
    "i = 1\n",
    "for word in word_freq:\n",
    "    if word_freq[word] > min_freq:\n",
    "        word_dict[word] = i\n",
    "        i += 1\n",
    "    else:\n",
    "        word_dict[word] = 0\n",
    "\n",
    "# dictionary length        \n",
    "dict_length = max(word_dict.values()) + 1\n",
    "dict_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f1104-da99-4de9-b060-60a09176bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to collate the tensors into batches, sentence need to be the same size\n",
    "# we could overwrite the collate function, or we could pick a max sentence size and pad\n",
    "\n",
    "max_length = 0\n",
    "for idx in tqdm(range(len(df_clean))):\n",
    "    row = df_clean.iloc[idx]\n",
    "    length = len(row['cleaned'].split(' '))\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809a29f-8a95-4848-b654-f968699afa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, df, word_dict, max_length):\n",
    "        self.df = df\n",
    "        self.word_dict = word_dict\n",
    "        self.sent_dict = {'negative': 0, 'positive': 1}\n",
    "        self.max_len = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        review = row['cleaned'].split(' ')\n",
    "        x = torch.zeros(self.max_len)\n",
    "        \n",
    "        # get review as a list of integers\n",
    "        for idx in range(len(review)):\n",
    "            \n",
    "            # we want to front pad for RNN\n",
    "            x[self.max_len - len(review) + idx] = self.word_dict[review[idx]]\n",
    "            \n",
    "        y = torch.tensor(self.sent_dict[row['sentiment']]).float()\n",
    "        \n",
    "        # embedding likes long tensors\n",
    "        return x.long(), y\n",
    "ds = IMDBDataset(df_clean, word_dict, max_length)\n",
    "next(iter(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6334c1-2566-46f8-801e-c9a0388b237b",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad95a1-5e42-40a4-8dfe-cf451764e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW model for sentiment analysis\n",
    "# train the embedding during training\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, dict_length, embedding_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        # padding index turns off gradient for unknown tokens\n",
    "        self.word_emb = nn.Embedding(dict_length, embedding_size, padding_idx=0)\n",
    "        self.linear = nn.Linear(embedding_size, 1)\n",
    "        self.emb_size = embedding_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sent_length = x.shape[1]\n",
    "        x = self.word_emb(x)\n",
    "        sent_length = torch.count_nonzero(x, dim=1)\n",
    "        x = torch.sum(x, dim=1) / sent_length\n",
    "        x = self.linear(x)\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19937a9-5719-4353-88ed-ffe765c224a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=1000, shuffle=True)\n",
    "x, y = next(iter(dl))\n",
    "\n",
    "cbow_model = CBOW(dict_length, 100)\n",
    "cbow_model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a5d08-e77d-4f61-a619-98ddf4f86278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_pass(model, dataloader, optimizer, lossFun, backwards=True, print_loss=False):\n",
    "    \n",
    "    if backwards == True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = lossFun(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if backwards == True:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if print_loss == True:\n",
    "        print(avg_loss)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def one_pass_acc(model, dataloader, num_points):\n",
    "    model.eval()\n",
    "    total_incorrect = 0\n",
    "        \n",
    "    for x, y in dataloader:\n",
    "        y_pred = (torch.sigmoid(model(x)) > 0.5).float()\n",
    "        \n",
    "        total_incorrect += torch.count_nonzero(y - y_pred).item()\n",
    "        \n",
    "    percent_wrong = total_incorrect / num_points\n",
    "    return 1 - percent_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a781becb-06fe-4914-965d-3c486b6179b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFun = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr = 0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print('Epoch: ', epoch)\n",
    "    \n",
    "    loss = one_pass(cbow_model, dl, optimizer, lossFun)\n",
    "    print('Loss: ', loss)\n",
    "    \n",
    "    acc = one_pass_acc(cbow_model, dl, len(ds))\n",
    "    print('Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e6805-c711-4c54-af43-1b892c2b0d91",
   "metadata": {},
   "source": [
    "## HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9779e2b-be46-4c14-bd35-3b42e1d5ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main HuggingFace package\n",
    "import transformers as hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2204b65-c2d2-40cb-8075-237ada130012",
   "metadata": {},
   "source": [
    "Note that the tokenizer is often intimately linked to the model you are using.\n",
    "- Below is the tokenizer for the basic BERT model (see [here](https://huggingface.co/bert-base-uncased))\n",
    "- Note that lemmatization and stop word removal techniques are not used here\n",
    "- You can also see some sub-word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f0d38-fd59-44c0-95f0-bb4e3c1ac279",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = hf.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sent = 'hello world, I am flying to Kashyyk!'\n",
    "\n",
    "tokenizer.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262d919-b548-4060-8041-21c78e49843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the output here\n",
    "tokenizer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3efeee-8148-449d-abed-63c5dfbba76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's going on here?\n",
    "tokenizer('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15792324-2777-4baf-8b34-e591c9864d34",
   "metadata": {},
   "source": [
    "Special tokens for beginning and end of sequences.\n",
    "- \"CLS\" for classification\n",
    "- \"SEP\" for separating (sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7542cd-9fb4-45b6-bfe4-9c681b941958",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 102])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd2b87-b361-42d9-8047-c3463f9752e8",
   "metadata": {},
   "source": [
    "Recall that all input sequences have to be the same length, so we often must use padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58131c-8036-4c66-8369-77c0498eda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic padding\n",
    "tokenizer(sent, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d9185-dd50-4d22-9611-6b6d3bbeac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad by longest sentence in list\n",
    "tokenizer(['a sentence', 'a longer sentence'], padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb844623-442a-4488-82b3-54f2742fbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy\n",
    "tokenizer(['a sentence',\n",
    "           'a longer sentence',\n",
    "           'a way way way longer sentence'], truncation=True, max_length=6, padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a933d-b0a8-4298-bc48-da15270f9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return PyTorch tensors!\n",
    "tokenizer(sent, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fb637-afa6-49ef-8b1c-d5e3830c4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can handle more than one sequence\n",
    "# notice the token type ids\n",
    "two_sents = tokenizer('here is a sentence', 'this is the second sentence')\n",
    "two_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5824cf-37eb-4769-bb36-0f66338ef687",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(two_sents['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409033a-3d76-4e64-9845-bc3a30940112",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hf.AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba362ea-894e-4341-9607-d9e839c10ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc2782-d3e4-4997-aaf1-89394cafca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no specified task here\n",
    "inputs = tokenizer(sent, padding='max_length', return_tensors=\"pt\")\n",
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1607b-0e02-48b6-b138-662929cb3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two output vectors\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968652c-5780-432b-a429-03a24b099775",
   "metadata": {},
   "source": [
    "The first output is the BERT emebedding for each token. The second output has pooled them together in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebe2dc-f51e-414c-ab33-e91177c79b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape, output[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6bb01-b437-4b49-b7c2-98b1b92494be",
   "metadata": {},
   "source": [
    "Now we're getting into the nitty-gritty of transformers. Let's take a step back in abstraction. What if I want to do something specific with a nice big, pretrained BERT model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af91cb-7100-4033-9b8c-cb54f56d1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification of sentences\n",
    "model = hf.AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388a8c7-c244-4b4c-a264-885bf31c49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = hf.AutoTokenizer.from_pretrained(name)\n",
    "model = hf.AutoModelForSequenceClassification.from_pretrained(name)\n",
    "\n",
    "sentences = [\"I am very angry\", \"I am very happy\", \"I am in the middle\"]\n",
    "\n",
    "tokens = tokenizer(sentences , padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "output = model(**tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13f30b-5416-4cbc-a6d0-8ef024f5f217",
   "metadata": {},
   "source": [
    "Let's abstract it even more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628a011-d316-4105-a2af-2f4b9e3c2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = hf.pipeline(\"sentiment-analysis\", model=name)\n",
    "classifier(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0e079-99e2-4f42-8b7a-10911041981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997749c6-711b-4bd9-8c16-924c0a943397",
   "metadata": {},
   "source": [
    "- As always beware of the [bias](https://huggingface.co/course/chapter1/8?fw=pt) in this model!\n",
    "- More [tasks](https://huggingface.co/course/chapter7/1?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75056416-d496-4788-8b5f-f8db65ca6785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
