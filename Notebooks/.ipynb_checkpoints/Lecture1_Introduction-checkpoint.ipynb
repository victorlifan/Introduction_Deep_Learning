{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(As always a new environment specifically for this course is recommended!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Data in PyTorch\n",
    "\n",
    "The torch ```tensor``` is the fundamental datatype used by PyTorch\n",
    "- works very similar to arrays\n",
    "- designed to work with GPUs\n",
    "- optimized for automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0],\n",
       "        [0, 1]]),\n",
       " tensor([[1, 0],\n",
       "         [0, 1]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# array\n",
    "X_array = np.array([[1,0],[0,1]])\n",
    "\n",
    "# tensor\n",
    "X_tensor = torch.tensor([[1,0],[0,1]])\n",
    "\n",
    "X_array, X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0],\n",
       "        [0, 1]]),\n",
       " tensor([[1, 0],\n",
       "         [0, 1]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can easily convert back and forth\n",
    "X_tensor.numpy(), torch.from_numpy(X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use a GPU, you can use one for free in Google Colab or Kaggle for a limited amount of time each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a GPU is available we can send the tensor to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(0)\n",
    "    X_tensor_cuda = X_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Dataset``` is an abstract class which holds the recipe for producing your data\n",
    "- can do complex operations to retrieve/transform your data in parallel\n",
    "- You must implement the following methods:\n",
    " - ```__init__```\n",
    " - ```__len__```: length of the dataset\n",
    " - ```__getitem__```: recipe for retrieving the *i*-th datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# create some linear data on [0,10] according to a slope, intercept, and number of desired points\n",
    "def random_linear_data(m, b, n):\n",
    "    x = 10 * np.random.rand(n)\n",
    "    y = m * x + b + np.random.rand(n)\n",
    "    return x, y\n",
    "\n",
    "# create a dataset class\n",
    "class LinearDataset(Dataset):\n",
    "    # things I need to intialize\n",
    "    def __init__(self, m, b, n):\n",
    "        x, y = random_linear_data(m, b, n)\n",
    "        self.x, self.y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "        self.n = n\n",
    "        \n",
    "    # length of the dataset\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    # how to get a datapoint\n",
    "    # any transformations you want to do on-the-fly\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "linear_ds = LinearDataset(1.5, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.2901, dtype=torch.float64), tensor(56.6574, dtype=torch.float64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get first datapoint\n",
    "next(iter(linear_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "# iterate through the dataset\n",
    "for x, y in linear_ds:\n",
    "    X.append(x.item())\n",
    "    Y.append(y.item())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn iris data into a Dataset\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "iris = iris[iris.species != 'virginica']\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.1000, 3.5000]), tensor(0.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.species_val = {'setosa':0,\n",
    "                            'versicolor':1}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # everything in getitem is done \"on-the-fly\"\n",
    "        \n",
    "        row = self.df.iloc[idx]\n",
    "        x = torch.tensor([row['sepal_length'],\n",
    "                          row['sepal_width']]).float()\n",
    "        \n",
    "        y = torch.tensor(self.species_val[row['species']]).float()\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "iris_ds = IrisDataset(iris)\n",
    "next(iter(iris_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general a good rule of thumb for what to do on-the-fly vs. preprocessing:\n",
    "- If it is random alteration (data augmentation): on-the-fly\n",
    "- If it is a time-consuming step that is also the same each time: preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Dataloader``` helps us iterate over a Dataset\n",
    "- can choose batch size\n",
    "- can shuffle\n",
    "- can be retrieved in parallel\n",
    "- automatically collates tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "iris_dl = DataLoader(iris_ds, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2]) torch.Size([10])\n",
      "tensor([[5.0000, 3.4000],\n",
      "        [6.3000, 2.3000],\n",
      "        [4.6000, 3.2000],\n",
      "        [5.2000, 4.1000],\n",
      "        [5.1000, 3.4000],\n",
      "        [5.7000, 2.6000],\n",
      "        [5.0000, 3.6000],\n",
      "        [5.1000, 3.5000],\n",
      "        [6.1000, 2.9000],\n",
      "        [5.8000, 2.7000]]) tensor([0., 1., 0., 0., 0., 1., 0., 0., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(iris_dl))\n",
    "print(x.shape, y.shape)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Model\n",
    "\n",
    "Let's define a simple Feed Forward neural network for the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        \n",
    "        # initialize the layers with random weights\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define the actual function\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # don't worry about the last activation function for now\n",
    "        return torch.squeeze(x)\n",
    "        \n",
    "model = TwoLayerNN(2, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoLayerNN(\n",
      "  (linear1): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (linear2): Linear(in_features=5, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the attached gradient function below. PyTorch autograd is keeping track of the computational graph for computing partial derivatives with respect to the various parameters/weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0938, 0.1430, 0.1442, 0.1730, 0.1315, 0.1282, 0.0778, 0.1391, 0.0816,\n",
       "         0.0778], grad_fn=<SqueezeBackward0>),\n",
       " tensor([0., 1., 1., 1., 1., 1., 0., 1., 0., 0.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(iris_dl))\n",
    "model(x), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some very useful tools for looking at models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                    [-1, 5]              15\n",
      "              ReLU-2                    [-1, 5]               0\n",
      "            Linear-3                    [-1, 1]               6\n",
      "================================================================\n",
      "Total params: 21\n",
      "Trainable params: 21\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size = (2,), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x3 and 2x5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bbf51312138c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# uh oh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/bc/lib/python3.9/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bc/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-938cf23356ce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# define the actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bc/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bc/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x3 and 2x5)"
     ]
    }
   ],
   "source": [
    "# uh oh\n",
    "summary(model, input_size = (3,), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.49.3 (20211023.0002)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"342pt\" height=\"480pt\"\n",
       " viewBox=\"0.00 0.00 342.00 480.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 476)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-476 338,-476 338,4 -4,4\"/>\n",
       "<!-- 6244964448 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>6244964448</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"193.5,-31 139.5,-31 139.5,0 193.5,0 193.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (10)</text>\n",
       "</g>\n",
       "<!-- 6244971808 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>6244971808</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"170,-92 57,-92 57,-73 170,-73 170,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.5\" y=\"-80\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 6244971808&#45;&gt;6244964448 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>6244971808&#45;&gt;6244964448</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M120.65,-72.73C127.65,-64.15 138.62,-50.69 148.07,-39.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"151,-41.05 154.61,-31.08 145.58,-36.62 151,-41.05\"/>\n",
       "</g>\n",
       "<!-- 6244972528 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>6244972528</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"217,-153 116,-153 116,-134 217,-134 217,-153\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-141\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 6244972528&#45;&gt;6244971808 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>6244972528&#45;&gt;6244971808</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.68,-133.79C150.69,-124.91 138.11,-110.89 128.24,-99.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"130.64,-97.34 121.35,-92.24 125.43,-102.02 130.64,-97.34\"/>\n",
       "</g>\n",
       "<!-- 4638262032 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>4638262032</title>\n",
       "<polygon fill=\"#a2cd5a\" stroke=\"black\" points=\"253,-98 188,-98 188,-67 253,-67 253,-98\"/>\n",
       "<text text-anchor=\"middle\" x=\"220.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\"> (10, 1)</text>\n",
       "</g>\n",
       "<!-- 6244972528&#45;&gt;4638262032 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>6244972528&#45;&gt;4638262032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174.47,-133.79C181.23,-126.41 191.23,-115.49 200.18,-105.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"202.88,-107.93 207.05,-98.19 197.72,-103.21 202.88,-107.93\"/>\n",
       "</g>\n",
       "<!-- 6244972720 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>6244972720</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-208 0,-208 0,-189 101,-189 101,-208\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-196\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6244972720&#45;&gt;6244972528 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>6244972720&#45;&gt;6244972528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.14,-188.98C87.8,-180.46 116.75,-167.23 138.24,-157.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"139.88,-160.51 147.52,-153.17 136.97,-154.14 139.88,-160.51\"/>\n",
       "</g>\n",
       "<!-- 6244041328 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>6244041328</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"95,-274 6,-274 6,-244 95,-244 95,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-262\" font-family=\"monospace\" font-size=\"10.00\">linear2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-251\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 6244041328&#45;&gt;6244972720 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>6244041328&#45;&gt;6244972720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-243.84C50.5,-236.21 50.5,-226.7 50.5,-218.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-218.27 50.5,-208.27 47,-218.27 54,-218.27\"/>\n",
       "</g>\n",
       "<!-- 6244972624 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>6244972624</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-208 119,-208 119,-189 214,-189 214,-208\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-196\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 6244972624&#45;&gt;6244972528 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>6244972624&#45;&gt;6244972528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.5,-188.75C166.5,-181.8 166.5,-171.85 166.5,-163.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170,-163.09 166.5,-153.09 163,-163.09 170,-163.09\"/>\n",
       "</g>\n",
       "<!-- 6244972672 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6244972672</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"215,-268.5 114,-268.5 114,-249.5 215,-249.5 215,-268.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-256.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 6244972672&#45;&gt;6244972624 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>6244972672&#45;&gt;6244972624</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.8,-249.37C165.07,-241.25 165.5,-228.81 165.85,-218.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.36,-218.28 166.2,-208.17 162.36,-218.04 169.36,-218.28\"/>\n",
       "</g>\n",
       "<!-- 6244972384 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6244972384</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"111,-334.5 10,-334.5 10,-315.5 111,-315.5 111,-334.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"60.5\" y=\"-322.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6244972384&#45;&gt;6244972672 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6244972384&#45;&gt;6244972672</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M74.53,-315.37C91.74,-304.78 121.16,-286.67 141.65,-274.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"143.68,-276.92 150.36,-268.7 140.01,-270.96 143.68,-276.92\"/>\n",
       "</g>\n",
       "<!-- 6244041168 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>6244041168</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"104,-406 15,-406 15,-376 104,-376 104,-406\"/>\n",
       "<text text-anchor=\"middle\" x=\"59.5\" y=\"-394\" font-family=\"monospace\" font-size=\"10.00\">linear1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"59.5\" y=\"-383\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 6244041168&#45;&gt;6244972384 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6244041168&#45;&gt;6244972384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.72,-375.8C59.86,-366.7 60.05,-354.79 60.2,-344.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.71,-344.9 60.36,-334.84 56.71,-344.79 63.71,-344.9\"/>\n",
       "</g>\n",
       "<!-- 6244972432 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>6244972432</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"209,-334.5 132,-334.5 132,-315.5 209,-315.5 209,-334.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-322.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 6244972432&#45;&gt;6244972672 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>6244972432&#45;&gt;6244972672</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.69,-315.37C168.82,-306.07 167.4,-290.98 166.27,-278.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.75,-278.53 165.33,-268.91 162.78,-279.19 169.75,-278.53\"/>\n",
       "</g>\n",
       "<!-- 6244972336 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>6244972336</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"223,-400.5 122,-400.5 122,-381.5 223,-381.5 223,-400.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-388.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6244972336&#45;&gt;6244972432 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>6244972336&#45;&gt;6244972432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172.23,-381.37C171.94,-372.16 171.48,-357.29 171.1,-345.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.59,-344.79 170.78,-334.91 167.59,-345.01 174.59,-344.79\"/>\n",
       "</g>\n",
       "<!-- 6244041008 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>6244041008</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"223,-472 122,-472 122,-442 223,-442 223,-472\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-460\" font-family=\"monospace\" font-size=\"10.00\">linear1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-449\" font-family=\"monospace\" font-size=\"10.00\"> (5, 2)</text>\n",
       "</g>\n",
       "<!-- 6244041008&#45;&gt;6244972336 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>6244041008&#45;&gt;6244972336</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172.5,-441.8C172.5,-432.7 172.5,-420.79 172.5,-410.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"176,-410.84 172.5,-400.84 169,-410.84 176,-410.84\"/>\n",
       "</g>\n",
       "<!-- 6244972816 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>6244972816</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"315,-208 238,-208 238,-189 315,-189 315,-208\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.5\" y=\"-196\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 6244972816&#45;&gt;6244972528 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>6244972816&#45;&gt;6244972528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.83,-188.98C241.21,-180.5 213.92,-167.35 193.57,-157.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"195.02,-154.36 184.49,-153.17 191.98,-160.66 195.02,-154.36\"/>\n",
       "</g>\n",
       "<!-- 6244972288 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>6244972288</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"334,-268.5 233,-268.5 233,-249.5 334,-249.5 334,-268.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"283.5\" y=\"-256.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6244972288&#45;&gt;6244972816 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>6244972288&#45;&gt;6244972816</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M282.47,-249.37C281.5,-241.25 280.01,-228.81 278.76,-218.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"282.2,-217.68 277.54,-208.17 275.25,-218.51 282.2,-217.68\"/>\n",
       "</g>\n",
       "<!-- 6244041248 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>6244041248</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"334,-340 233,-340 233,-310 334,-310 334,-340\"/>\n",
       "<text text-anchor=\"middle\" x=\"283.5\" y=\"-328\" font-family=\"monospace\" font-size=\"10.00\">linear2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"283.5\" y=\"-317\" font-family=\"monospace\" font-size=\"10.00\"> (1, 5)</text>\n",
       "</g>\n",
       "<!-- 6244041248&#45;&gt;6244972288 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>6244041248&#45;&gt;6244972288</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M283.5,-309.8C283.5,-300.7 283.5,-288.79 283.5,-278.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"287,-278.84 283.5,-268.84 280,-278.84 287,-278.84\"/>\n",
       "</g>\n",
       "<!-- 4638262032&#45;&gt;6244964448 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>4638262032&#45;&gt;6244964448</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M208.25,-66.75C201.41,-58.52 192.76,-48.11 185.1,-38.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"187.79,-36.65 178.7,-31.19 182.4,-41.12 187.79,-36.65\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1743ab550>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "make_dot(model(x), params=dict(list(model.named_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the function is straightforward we can just use Sequential\n",
    "#model = nn.Sequential(nn.Linear(2, 5),\n",
    "#                      nn.ReLU(),\n",
    "#                      nn.Linear(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "We need the following ingredients\n",
    "- A loss function for our model\n",
    "- An optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# feeds outputs through a sigmoid before computing BCE Loss\n",
    "lossFun = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we adjust the weights of the model according to one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67100989818573\n"
     ]
    }
   ],
   "source": [
    "# adjust the gradients according to one batch\n",
    "\n",
    "x, y = next(iter(iris_dl))\n",
    "\n",
    "# some layers will do different things during training/prediction (i.e. dropout)\n",
    "model.train()\n",
    "\n",
    "# compute the predictions then loss\n",
    "y_pred = model(x)\n",
    "loss = lossFun(y_pred, y)\n",
    "print(loss.item())\n",
    "\n",
    "# zero out the gradients in the optimizer (otherwise they will accumulate)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# compute the gradients w.r.t. loss function\n",
    "loss.backward()\n",
    "\n",
    "# adjust weights!\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *epoch* is one pass through the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6677646636962891\n",
      "0.6794701814651489\n",
      "0.705277144908905\n",
      "0.6713263392448425\n",
      "0.6921883225440979\n",
      "0.6793587803840637\n",
      "0.6667422652244568\n",
      "0.6894742250442505\n",
      "0.6668645143508911\n",
      "0.6789766550064087\n",
      "0.6664751172065735\n",
      "0.6898847818374634\n",
      "0.665474534034729\n",
      "0.6763577461242676\n",
      "0.6789048910140991\n",
      "0.6923978924751282\n",
      "0.6818576455116272\n",
      "0.6920389533042908\n",
      "0.6697114109992981\n",
      "0.6751593351364136\n",
      "0.6760469675064087\n",
      "0.6562336683273315\n",
      "0.676021933555603\n",
      "0.6699674725532532\n",
      "0.7164481282234192\n",
      "0.6757311224937439\n",
      "0.7013643980026245\n",
      "0.7132371068000793\n",
      "0.6308371424674988\n",
      "0.663428783416748\n",
      "0.6915148496627808\n",
      "0.6913242936134338\n",
      "0.6758570671081543\n",
      "0.6423933506011963\n",
      "0.6851816177368164\n",
      "0.6893893480300903\n",
      "0.6780350804328918\n",
      "0.6765543222427368\n",
      "0.6529719233512878\n",
      "0.686550498008728\n",
      "0.665449857711792\n",
      "0.7013241648674011\n",
      "0.6495400071144104\n",
      "0.6681531071662903\n",
      "0.6738815903663635\n",
      "0.6640737056732178\n",
      "0.6521415114402771\n",
      "0.7093033790588379\n",
      "0.6779900789260864\n",
      "0.7002878189086914\n",
      "0.6563071012496948\n",
      "0.6506032347679138\n",
      "0.6711308360099792\n",
      "0.7024675011634827\n",
      "0.7147595286369324\n",
      "0.6972241997718811\n",
      "0.6628845930099487\n",
      "0.6610703468322754\n",
      "0.6858475804328918\n",
      "0.6514164209365845\n",
      "0.6973044276237488\n",
      "0.649768054485321\n",
      "0.6889210343360901\n",
      "0.6720221042633057\n",
      "0.6581042408943176\n",
      "0.6753063201904297\n",
      "0.6624529957771301\n",
      "0.6906675100326538\n",
      "0.6723396182060242\n",
      "0.6773120164871216\n",
      "0.6470240354537964\n",
      "0.7000623345375061\n",
      "0.6954463720321655\n",
      "0.6626638174057007\n",
      "0.6982288956642151\n",
      "0.6880449056625366\n",
      "0.6575899124145508\n",
      "0.6690763235092163\n",
      "0.6287927031517029\n",
      "0.6890395879745483\n",
      "0.6745865345001221\n",
      "0.685920238494873\n",
      "0.6737130284309387\n",
      "0.6597572565078735\n",
      "0.6562930345535278\n",
      "0.6853007078170776\n",
      "0.6735655069351196\n",
      "0.6635457277297974\n",
      "0.6661585569381714\n",
      "0.6866956353187561\n",
      "0.6932458877563477\n",
      "0.6721287965774536\n",
      "0.6713654398918152\n",
      "0.6816325187683105\n",
      "0.6724218726158142\n",
      "0.6376858949661255\n",
      "0.6738856434822083\n",
      "0.6804115176200867\n",
      "0.6717621088027954\n",
      "0.6619028449058533\n",
      "0.662650465965271\n",
      "0.6744291186332703\n",
      "0.69351726770401\n",
      "0.6703159809112549\n",
      "0.668234646320343\n",
      "0.6710920929908752\n",
      "0.634463906288147\n",
      "0.6438034772872925\n",
      "0.6970390677452087\n",
      "0.690599262714386\n",
      "0.6630856990814209\n",
      "0.6590023040771484\n",
      "0.6940938234329224\n",
      "0.679316520690918\n",
      "0.665042519569397\n",
      "0.667786180973053\n",
      "0.639881432056427\n",
      "0.6683791279792786\n",
      "0.6775288581848145\n",
      "0.6811544299125671\n",
      "0.6764615774154663\n",
      "0.678861677646637\n",
      "0.6724710464477539\n",
      "0.6703618764877319\n",
      "0.6896907091140747\n",
      "0.6582375764846802\n",
      "0.685673177242279\n",
      "0.6572253108024597\n",
      "0.6521428227424622\n",
      "0.644493579864502\n",
      "0.668444812297821\n",
      "0.6871293187141418\n",
      "0.6587264537811279\n",
      "0.6683111786842346\n",
      "0.637992262840271\n",
      "0.6808271408081055\n",
      "0.6725479364395142\n",
      "0.6394511461257935\n",
      "0.675888180732727\n",
      "0.6836798787117004\n",
      "0.677216112613678\n",
      "0.6619677543640137\n",
      "0.6909044981002808\n",
      "0.6431765556335449\n",
      "0.6379682421684265\n",
      "0.671241044998169\n",
      "0.6385392546653748\n",
      "0.7018448114395142\n",
      "0.6757680773735046\n",
      "0.6631187796592712\n",
      "0.6518049240112305\n",
      "0.6967863440513611\n",
      "0.66664057970047\n",
      "0.6432730555534363\n",
      "0.6375952959060669\n",
      "0.6398321390151978\n",
      "0.6840798258781433\n",
      "0.6823066473007202\n",
      "0.6680206060409546\n",
      "0.6798283457756042\n",
      "0.6844717860221863\n",
      "0.6625515222549438\n",
      "0.6709171533584595\n",
      "0.6713569760322571\n",
      "0.6371058821678162\n",
      "0.6619833707809448\n",
      "0.669169545173645\n",
      "0.6428267955780029\n",
      "0.6724451780319214\n",
      "0.6652527451515198\n",
      "0.6714707612991333\n",
      "0.6685770153999329\n",
      "0.6607691645622253\n",
      "0.6417344808578491\n",
      "0.6604933738708496\n",
      "0.6360672116279602\n",
      "0.6660120487213135\n",
      "0.661501407623291\n",
      "0.6711356043815613\n",
      "0.6871669888496399\n",
      "0.6571776866912842\n",
      "0.6385101079940796\n",
      "0.6702488660812378\n",
      "0.6645882725715637\n",
      "0.6730297803878784\n",
      "0.6501995921134949\n",
      "0.6730290651321411\n",
      "0.6463844180107117\n",
      "0.687487781047821\n",
      "0.6507860422134399\n",
      "0.628184974193573\n",
      "0.6846004724502563\n",
      "0.6590226888656616\n",
      "0.6869085431098938\n",
      "0.6735424995422363\n",
      "0.6542080640792847\n",
      "0.6889216899871826\n",
      "0.6355004906654358\n",
      "0.6587039232254028\n",
      "0.6300674676895142\n",
      "0.646511971950531\n",
      "0.6827125549316406\n",
      "0.6642276048660278\n",
      "0.6881110668182373\n",
      "0.6521319150924683\n",
      "0.6311696767807007\n",
      "0.6638730764389038\n",
      "0.6601960062980652\n",
      "0.6371712684631348\n",
      "0.6597916483879089\n",
      "0.6789025664329529\n",
      "0.6437782049179077\n",
      "0.63703453540802\n",
      "0.6536425352096558\n",
      "0.6462818384170532\n",
      "0.6803959608078003\n",
      "0.6511395573616028\n",
      "0.6532465815544128\n",
      "0.6493285894393921\n",
      "0.6792240738868713\n",
      "0.6727019548416138\n",
      "0.6600415110588074\n",
      "0.6476987600326538\n",
      "0.6659308075904846\n",
      "0.634907066822052\n",
      "0.6699849367141724\n",
      "0.6291531920433044\n",
      "0.6614318490028381\n",
      "0.658164381980896\n",
      "0.6586385369300842\n",
      "0.6509254574775696\n",
      "0.6267215013504028\n",
      "0.6867558360099792\n",
      "0.6340978145599365\n",
      "0.6433736085891724\n",
      "0.6811321973800659\n",
      "0.6697893738746643\n",
      "0.6758190393447876\n",
      "0.6491645574569702\n",
      "0.6283890008926392\n",
      "0.6274605989456177\n",
      "0.6744168400764465\n",
      "0.6672523617744446\n",
      "0.6446473002433777\n",
      "0.6494086980819702\n",
      "0.6586543321609497\n",
      "0.6484538912773132\n",
      "0.6628397703170776\n",
      "0.6401036977767944\n",
      "0.6581836938858032\n",
      "0.6605697870254517\n",
      "0.6692481637001038\n",
      "0.6483200192451477\n",
      "0.6486453413963318\n",
      "0.6519883871078491\n",
      "0.616683840751648\n",
      "0.6251314878463745\n",
      "0.6727646589279175\n",
      "0.6700786352157593\n",
      "0.6545030474662781\n",
      "0.6544338464736938\n",
      "0.6329823136329651\n",
      "0.7007129192352295\n",
      "0.6344200372695923\n",
      "0.6513954997062683\n",
      "0.6258370876312256\n",
      "0.6329678297042847\n",
      "0.6808343529701233\n",
      "0.6496151685714722\n",
      "0.6402415633201599\n",
      "0.6677042245864868\n",
      "0.6530791521072388\n",
      "0.6269081830978394\n",
      "0.6657300591468811\n",
      "0.6452833414077759\n",
      "0.6940581202507019\n",
      "0.659786581993103\n",
      "0.6359592080116272\n",
      "0.6198952794075012\n",
      "0.6209070086479187\n",
      "0.6546104550361633\n",
      "0.6402065753936768\n",
      "0.6405736804008484\n",
      "0.6455892324447632\n",
      "0.6645820140838623\n",
      "0.6413052678108215\n",
      "0.6633008122444153\n",
      "0.6314376592636108\n",
      "0.6625378131866455\n",
      "0.6299037933349609\n",
      "0.6394147276878357\n",
      "0.6345149278640747\n",
      "0.625652015209198\n",
      "0.6792201995849609\n",
      "0.6307904720306396\n",
      "0.6527230143547058\n",
      "0.658865213394165\n",
      "0.6410897970199585\n",
      "0.6339679956436157\n",
      "0.6637917160987854\n",
      "0.6185402274131775\n",
      "0.6604140996932983\n",
      "0.6668688654899597\n",
      "0.6595801711082458\n",
      "0.628494381904602\n",
      "0.6293807029724121\n",
      "0.6567438840866089\n",
      "0.674212634563446\n",
      "0.6147407293319702\n",
      "0.6347110271453857\n",
      "0.6373893618583679\n",
      "0.6396048665046692\n",
      "0.6328204870223999\n",
      "0.6442784667015076\n",
      "0.651721179485321\n",
      "0.6515970230102539\n",
      "0.6407058835029602\n",
      "0.6508828401565552\n",
      "0.6153850555419922\n",
      "0.6638458371162415\n",
      "0.6680694818496704\n",
      "0.6383088827133179\n",
      "0.6656678915023804\n",
      "0.6261836886405945\n",
      "0.6596660017967224\n",
      "0.6364099383354187\n",
      "0.6142767667770386\n",
      "0.6355641484260559\n",
      "0.6318361163139343\n",
      "0.6375771164894104\n",
      "0.6407278776168823\n",
      "0.6425797939300537\n",
      "0.6338089108467102\n",
      "0.6408867239952087\n",
      "0.6653301119804382\n",
      "0.6325567960739136\n",
      "0.6325777769088745\n",
      "0.6018264293670654\n",
      "0.6481952667236328\n",
      "0.6582998633384705\n",
      "0.6290491819381714\n",
      "0.6344124674797058\n",
      "0.6072026491165161\n",
      "0.6463930606842041\n",
      "0.6301857233047485\n",
      "0.6651538610458374\n",
      "0.6604474782943726\n",
      "0.6281875371932983\n",
      "0.6779413223266602\n",
      "0.6018195748329163\n",
      "0.6514091491699219\n",
      "0.6235758066177368\n",
      "0.6467632055282593\n",
      "0.6273683905601501\n",
      "0.6473392844200134\n",
      "0.6317040920257568\n",
      "0.6371995806694031\n",
      "0.6182795763015747\n",
      "0.6375479698181152\n",
      "0.6419855356216431\n",
      "0.6189835667610168\n",
      "0.6311818957328796\n",
      "0.6229187250137329\n",
      "0.6508253812789917\n",
      "0.6074718236923218\n",
      "0.6641020774841309\n",
      "0.6538099050521851\n",
      "0.648563802242279\n",
      "0.6208332180976868\n",
      "0.6288121342658997\n",
      "0.6358413100242615\n",
      "0.6311365365982056\n",
      "0.605015218257904\n",
      "0.6157470941543579\n",
      "0.632357656955719\n",
      "0.6322636008262634\n",
      "0.6431318521499634\n",
      "0.6442955732345581\n",
      "0.6355231404304504\n",
      "0.6544790863990784\n",
      "0.6410176753997803\n",
      "0.6307476758956909\n",
      "0.6360617876052856\n",
      "0.6692520380020142\n",
      "0.6287178993225098\n",
      "0.6480817198753357\n",
      "0.6222659349441528\n",
      "0.6092140674591064\n",
      "0.6135143041610718\n",
      "0.6158775091171265\n",
      "0.685549259185791\n",
      "0.6579028964042664\n",
      "0.6464889645576477\n",
      "0.6229856610298157\n",
      "0.6238068342208862\n",
      "0.6257184743881226\n",
      "0.6394383311271667\n",
      "0.5900715589523315\n",
      "0.5954959988594055\n",
      "0.6132184267044067\n",
      "0.6358622312545776\n",
      "0.6115534901618958\n",
      "0.6025906801223755\n",
      "0.6470252275466919\n",
      "0.6272761225700378\n",
      "0.6463820934295654\n",
      "0.6318320631980896\n",
      "0.6005761027336121\n",
      "0.6181490421295166\n",
      "0.6574965715408325\n",
      "0.608022153377533\n",
      "0.6158727407455444\n",
      "0.6731204390525818\n",
      "0.6224223375320435\n",
      "0.5917843580245972\n",
      "0.61629319190979\n",
      "0.6392744183540344\n",
      "0.6203357577323914\n",
      "0.628848671913147\n",
      "0.6440967321395874\n",
      "0.6636940240859985\n",
      "0.6188024878501892\n",
      "0.5897234678268433\n",
      "0.6423430442810059\n",
      "0.6192194223403931\n",
      "0.6365931630134583\n",
      "0.5862280130386353\n",
      "0.6303699612617493\n",
      "0.630879282951355\n",
      "0.6240419149398804\n",
      "0.6530214548110962\n",
      "0.5850976705551147\n",
      "0.6249628067016602\n",
      "0.6054741144180298\n",
      "0.6489046812057495\n",
      "0.6176271438598633\n",
      "0.6471999883651733\n",
      "0.6047183871269226\n",
      "0.6470462679862976\n",
      "0.5896812677383423\n",
      "0.6156832575798035\n",
      "0.5974926948547363\n",
      "0.6043065786361694\n",
      "0.5934374332427979\n",
      "0.6520420908927917\n",
      "0.6611243486404419\n",
      "0.6406663060188293\n",
      "0.5880618095397949\n",
      "0.6316524744033813\n",
      "0.6236913800239563\n",
      "0.5725902915000916\n",
      "0.6182113885879517\n",
      "0.6068604588508606\n",
      "0.6235034465789795\n",
      "0.6333483457565308\n",
      "0.6220995783805847\n",
      "0.6117857098579407\n",
      "0.625270426273346\n",
      "0.6393555998802185\n",
      "0.6345019340515137\n",
      "0.61146080493927\n",
      "0.5784733891487122\n",
      "0.6085019111633301\n",
      "0.5944585800170898\n",
      "0.6244262456893921\n",
      "0.5946264863014221\n",
      "0.6675213575363159\n",
      "0.6103485822677612\n",
      "0.6530902981758118\n",
      "0.6299177408218384\n",
      "0.6323705315589905\n",
      "0.626738965511322\n",
      "0.5759193301200867\n",
      "0.6549516916275024\n",
      "0.6271177530288696\n",
      "0.5848736763000488\n",
      "0.6123541593551636\n",
      "0.6265887022018433\n",
      "0.6124311089515686\n",
      "0.5969575643539429\n",
      "0.5983491539955139\n",
      "0.6095075607299805\n",
      "0.6484554409980774\n",
      "0.6221444010734558\n",
      "0.5985637903213501\n",
      "0.6298482418060303\n",
      "0.6116625666618347\n",
      "0.6049922108650208\n",
      "0.596039891242981\n",
      "0.611538290977478\n",
      "0.5967057943344116\n",
      "0.6307185292243958\n",
      "0.5984400510787964\n",
      "0.6053760051727295\n",
      "0.5895565152168274\n",
      "0.662670373916626\n",
      "0.5844216346740723\n",
      "0.624334454536438\n",
      "0.5980618596076965\n",
      "0.6224585771560669\n",
      "0.6319559216499329\n",
      "0.6153186559677124\n",
      "0.625272810459137\n",
      "0.5609978437423706\n",
      "0.5738478302955627\n",
      "0.6428551077842712\n",
      "0.6201409101486206\n",
      "0.6469756364822388\n",
      "0.559108555316925\n",
      "0.6177532076835632\n",
      "0.6084573268890381\n",
      "0.6076217889785767\n",
      "0.5714660882949829\n",
      "0.5608463883399963\n",
      "0.6319629549980164\n",
      "0.598657488822937\n",
      "0.6097116470336914\n",
      "0.6708815097808838\n",
      "0.6237701177597046\n",
      "0.5937812328338623\n",
      "0.6289554834365845\n",
      "0.5781627297401428\n",
      "0.5610448122024536\n",
      "0.6069796681404114\n",
      "0.6341408491134644\n",
      "0.5971160531044006\n",
      "0.6411302089691162\n",
      "0.6363745927810669\n",
      "0.605411171913147\n",
      "0.5662044882774353\n",
      "0.6149851083755493\n",
      "0.593551516532898\n",
      "0.6002287268638611\n",
      "0.6611124277114868\n",
      "0.5665261149406433\n",
      "0.6143215298652649\n",
      "0.6001193523406982\n",
      "0.58064204454422\n",
      "0.5877352356910706\n",
      "0.6166008114814758\n",
      "0.6253560185432434\n",
      "0.5484051704406738\n",
      "0.5930992364883423\n",
      "0.5979577898979187\n",
      "0.6117087602615356\n",
      "0.6130198240280151\n",
      "0.5986719131469727\n",
      "0.5910176038742065\n",
      "0.6403335332870483\n",
      "0.5970658659934998\n",
      "0.614724338054657\n",
      "0.6201960444450378\n",
      "0.5721850395202637\n",
      "0.5872948169708252\n",
      "0.6226335763931274\n",
      "0.6463611125946045\n",
      "0.5730687379837036\n",
      "0.5898679494857788\n",
      "0.6019979119300842\n",
      "0.5706879496574402\n",
      "0.5948999524116516\n",
      "0.5768463015556335\n",
      "0.6267234086990356\n",
      "0.589934229850769\n",
      "0.5572344064712524\n",
      "0.547010600566864\n",
      "0.6361721754074097\n",
      "0.6076346635818481\n",
      "0.6583101153373718\n",
      "0.5831592082977295\n",
      "0.5824302434921265\n",
      "0.6385728716850281\n",
      "0.6050846576690674\n",
      "0.5957359671592712\n",
      "0.6201882362365723\n",
      "0.5841522216796875\n",
      "0.53401780128479\n",
      "0.6077428460121155\n",
      "0.569804847240448\n",
      "0.6195300817489624\n",
      "0.610766589641571\n",
      "0.5667890310287476\n",
      "0.6296392679214478\n",
      "0.6169663071632385\n",
      "0.5860875844955444\n",
      "0.5980779528617859\n",
      "0.542209267616272\n",
      "0.5817910432815552\n",
      "0.5651248097419739\n",
      "0.6386150121688843\n",
      "0.570115327835083\n",
      "0.6312965154647827\n",
      "0.6171786785125732\n",
      "0.546928882598877\n",
      "0.6228622794151306\n",
      "0.5741421580314636\n",
      "0.5696297287940979\n",
      "0.6104716062545776\n",
      "0.5530011653900146\n",
      "0.6201139688491821\n",
      "0.5927006602287292\n",
      "0.5555899143218994\n",
      "0.6003304719924927\n",
      "0.6333357691764832\n",
      "0.5736813545227051\n",
      "0.6078981161117554\n",
      "0.5811986327171326\n",
      "0.5879181027412415\n",
      "0.608415961265564\n",
      "0.5539907813072205\n",
      "0.6030015349388123\n",
      "0.6473290324211121\n",
      "0.5968016386032104\n",
      "0.5778385400772095\n",
      "0.57394939661026\n",
      "0.5259380340576172\n",
      "0.5702100396156311\n",
      "0.5437115430831909\n",
      "0.6174482107162476\n",
      "0.6206049919128418\n",
      "0.6246892213821411\n",
      "0.5498031377792358\n",
      "0.586986780166626\n",
      "0.6336209774017334\n",
      "0.6143556237220764\n",
      "0.6111240386962891\n",
      "0.5683362483978271\n",
      "0.5471409559249878\n",
      "0.559414267539978\n",
      "0.5620658993721008\n",
      "0.6260892748832703\n",
      "0.5827042460441589\n",
      "0.5993202328681946\n",
      "0.5769280195236206\n",
      "0.6048420667648315\n",
      "0.5385400652885437\n",
      "0.5953088998794556\n",
      "0.5871193408966064\n",
      "0.5618637204170227\n",
      "0.5609785914421082\n",
      "0.6230801343917847\n",
      "0.6153079271316528\n",
      "0.5815310478210449\n",
      "0.5985375642776489\n",
      "0.5174747109413147\n",
      "0.5628973245620728\n",
      "0.5645853877067566\n",
      "0.5794844031333923\n",
      "0.5795163512229919\n",
      "0.5909696817398071\n",
      "0.6009467840194702\n",
      "0.5750099420547485\n",
      "0.6253613233566284\n",
      "0.5596656799316406\n",
      "0.6173064112663269\n",
      "0.5922722816467285\n",
      "0.4944075644016266\n",
      "0.5180099606513977\n",
      "0.5858405828475952\n",
      "0.625822126865387\n",
      "0.5913251042366028\n",
      "0.65853351354599\n",
      "0.511581301689148\n",
      "0.5353055596351624\n",
      "0.6294174194335938\n",
      "0.6001385450363159\n",
      "0.5896937251091003\n",
      "0.5829647779464722\n",
      "0.5416013598442078\n",
      "0.5336284637451172\n",
      "0.6074435114860535\n",
      "0.5696700811386108\n",
      "0.618739128112793\n",
      "0.5979725122451782\n",
      "0.534140944480896\n",
      "0.5475308895111084\n",
      "0.5856058597564697\n",
      "0.5314196944236755\n",
      "0.5518258810043335\n",
      "0.6088767647743225\n",
      "0.5632501244544983\n",
      "0.5741373300552368\n",
      "0.5769459009170532\n",
      "0.5725737810134888\n",
      "0.5646399259567261\n",
      "0.519621729850769\n",
      "0.5571565628051758\n",
      "0.5976070761680603\n",
      "0.5713857412338257\n",
      "0.6379563212394714\n",
      "0.586654007434845\n",
      "0.5591232180595398\n",
      "0.5963032841682434\n",
      "0.5541881322860718\n",
      "0.5870696306228638\n",
      "0.5618561506271362\n",
      "0.625541090965271\n",
      "0.5264403223991394\n",
      "0.5899801254272461\n",
      "0.5236058235168457\n",
      "0.5468376874923706\n",
      "0.5364924669265747\n",
      "0.567192018032074\n",
      "0.5996646285057068\n",
      "0.598827064037323\n",
      "0.5769580602645874\n",
      "0.5208556652069092\n",
      "0.6104710698127747\n",
      "0.5764930844306946\n",
      "0.5565147399902344\n",
      "0.5040445327758789\n",
      "0.5659565925598145\n",
      "0.5685476064682007\n",
      "0.5587875247001648\n",
      "0.5843749046325684\n",
      "0.5569902062416077\n",
      "0.5941061973571777\n",
      "0.5772196650505066\n",
      "0.585593581199646\n",
      "0.5752773880958557\n",
      "0.5476089715957642\n",
      "0.5579782724380493\n",
      "0.5765804052352905\n",
      "0.5133284330368042\n",
      "0.5588029623031616\n",
      "0.5625861883163452\n",
      "0.518470287322998\n",
      "0.618634819984436\n",
      "0.6027877926826477\n",
      "0.5921279191970825\n",
      "0.5565398931503296\n",
      "0.5869226455688477\n",
      "0.5407371520996094\n",
      "0.5488740801811218\n",
      "0.5638421177864075\n",
      "0.5817839503288269\n",
      "0.5474776029586792\n",
      "0.5786087512969971\n",
      "0.5820698142051697\n",
      "0.5399669408798218\n",
      "0.5591316223144531\n",
      "0.5860004425048828\n",
      "0.5513521432876587\n",
      "0.6029057502746582\n",
      "0.5838775038719177\n",
      "0.6075690984725952\n",
      "0.5112813711166382\n",
      "0.5502234697341919\n",
      "0.5223240256309509\n",
      "0.5357546806335449\n",
      "0.5568102598190308\n",
      "0.5431723594665527\n",
      "0.5580791234970093\n",
      "0.5391231179237366\n",
      "0.4743056893348694\n",
      "0.5128549337387085\n",
      "0.6083077192306519\n",
      "0.5708686113357544\n",
      "0.5992132425308228\n",
      "0.6295657157897949\n",
      "0.5587770938873291\n",
      "0.5149418115615845\n",
      "0.5805148482322693\n",
      "0.6417580842971802\n",
      "0.5173371434211731\n",
      "0.5655516386032104\n",
      "0.5877761244773865\n",
      "0.5103138089179993\n",
      "0.5080633163452148\n",
      "0.5790641903877258\n",
      "0.5047670006752014\n",
      "0.5274857878684998\n",
      "0.5492153763771057\n",
      "0.5504175424575806\n",
      "0.5482520461082458\n",
      "0.5499550104141235\n",
      "0.5836741924285889\n",
      "0.6048111915588379\n",
      "0.5518047213554382\n",
      "0.5740249752998352\n",
      "0.5614610910415649\n",
      "0.554594874382019\n",
      "0.520260751247406\n",
      "0.6256685853004456\n",
      "0.5590253472328186\n",
      "0.522721529006958\n",
      "0.5244622826576233\n",
      "0.5832640528678894\n",
      "0.5573644638061523\n",
      "0.5127647519111633\n",
      "0.4606877267360687\n",
      "0.6001943945884705\n",
      "0.5826101899147034\n",
      "0.5527581572532654\n",
      "0.630935788154602\n",
      "0.5588981509208679\n",
      "0.5742681622505188\n",
      "0.4797971844673157\n",
      "0.5513875484466553\n",
      "0.5095472931861877\n",
      "0.49864545464515686\n",
      "0.5812403559684753\n",
      "0.5273666977882385\n",
      "0.5905441045761108\n",
      "0.588290274143219\n",
      "0.597878098487854\n",
      "0.4742816388607025\n",
      "0.5233217477798462\n",
      "0.5702389478683472\n",
      "0.5284482836723328\n",
      "0.5394313335418701\n",
      "0.5921648144721985\n",
      "0.5307033061981201\n",
      "0.5719699859619141\n",
      "0.5443824529647827\n",
      "0.5157319903373718\n",
      "0.5801740884780884\n",
      "0.5109619498252869\n",
      "0.4984320104122162\n",
      "0.5729125738143921\n",
      "0.5471197366714478\n",
      "0.5577501058578491\n",
      "0.6019781827926636\n",
      "0.5204364657402039\n",
      "0.589797854423523\n",
      "0.5259026885032654\n",
      "0.44175395369529724\n",
      "0.6107038259506226\n",
      "0.45552095770835876\n",
      "0.5850958228111267\n",
      "0.5379074811935425\n",
      "0.5798119306564331\n",
      "0.5199130773544312\n",
      "0.5539339184761047\n",
      "0.4613795280456543\n",
      "0.4823896288871765\n",
      "0.5003777146339417\n",
      "0.5863330960273743\n",
      "0.5703694224357605\n",
      "0.6265507340431213\n",
      "0.5406530499458313\n",
      "0.5981394052505493\n",
      "0.5334221720695496\n",
      "0.6017522215843201\n",
      "0.5045031905174255\n",
      "0.47151899337768555\n",
      "0.5327877998352051\n",
      "0.47514328360557556\n",
      "0.5534223914146423\n",
      "0.5827097296714783\n",
      "0.5081780552864075\n",
      "0.5749198198318481\n",
      "0.48237061500549316\n",
      "0.4942132830619812\n",
      "0.568187415599823\n",
      "0.514994740486145\n",
      "0.5071311593055725\n",
      "0.6034291982650757\n",
      "0.5786194205284119\n",
      "0.5392985939979553\n",
      "0.5095796585083008\n",
      "0.5494431257247925\n",
      "0.5648168325424194\n",
      "0.5232453942298889\n",
      "0.5509981513023376\n",
      "0.5446287989616394\n",
      "0.523026168346405\n",
      "0.5189467668533325\n",
      "0.5234907865524292\n",
      "0.5422252416610718\n",
      "0.597263514995575\n",
      "0.5224164724349976\n",
      "0.5358905792236328\n",
      "0.5152941346168518\n",
      "0.5973997712135315\n",
      "0.5960454940795898\n",
      "0.45631831884384155\n",
      "0.514916718006134\n",
      "0.486910343170166\n",
      "0.5069440603256226\n",
      "0.5892823338508606\n",
      "0.5040939450263977\n",
      "0.5235334038734436\n",
      "0.49806857109069824\n",
      "0.514253556728363\n",
      "0.49345818161964417\n",
      "0.549199104309082\n",
      "0.5765776634216309\n",
      "0.4837697446346283\n",
      "0.5746791362762451\n",
      "0.5011009573936462\n",
      "0.5765966773033142\n",
      "0.5235685706138611\n",
      "0.4621953070163727\n",
      "0.5792158246040344\n",
      "0.5304492712020874\n",
      "0.48339590430259705\n",
      "0.5981727242469788\n",
      "0.47752705216407776\n",
      "0.5529796481132507\n",
      "0.545884907245636\n",
      "0.5005185008049011\n",
      "0.46581822633743286\n",
      "0.5104698538780212\n",
      "0.5867487192153931\n",
      "0.5196687579154968\n",
      "0.5764726996421814\n",
      "0.5183793902397156\n",
      "0.5013908743858337\n",
      "0.5399609208106995\n",
      "0.5649962425231934\n",
      "0.5237604379653931\n",
      "0.49229541420936584\n",
      "0.5141276121139526\n",
      "0.5591570734977722\n",
      "0.5648844242095947\n",
      "0.4959375262260437\n",
      "0.5034842491149902\n",
      "0.494391530752182\n",
      "0.529499351978302\n",
      "0.5023834109306335\n",
      "0.5503097772598267\n",
      "0.5066213011741638\n",
      "0.4606911242008209\n",
      "0.518413245677948\n",
      "0.461647093296051\n",
      "0.5309712290763855\n",
      "0.5298205614089966\n",
      "0.5911747813224792\n",
      "0.5694929957389832\n",
      "0.5869110822677612\n",
      "0.4771898686885834\n",
      "0.4826291501522064\n",
      "0.4793737828731537\n",
      "0.4951508045196533\n",
      "0.5328232049942017\n",
      "0.5369480848312378\n",
      "0.6216030716896057\n",
      "0.4690360128879547\n",
      "0.5192240476608276\n",
      "0.5907108187675476\n",
      "0.47566384077072144\n",
      "0.4382588863372803\n",
      "0.4943420886993408\n",
      "0.6336025595664978\n",
      "0.4386308193206787\n",
      "0.4950721859931946\n",
      "0.5494868755340576\n",
      "0.5310347676277161\n",
      "0.5309436321258545\n",
      "0.5572123527526855\n",
      "0.5839207768440247\n",
      "0.5332797169685364\n",
      "0.5178173780441284\n",
      "0.5346692800521851\n",
      "0.5157262682914734\n",
      "0.4435881972312927\n",
      "0.49440550804138184\n",
      "0.4622820317745209\n",
      "0.5207709670066833\n",
      "0.5217059850692749\n",
      "0.46845149993896484\n",
      "0.5678220987319946\n",
      "0.4216367304325104\n",
      "0.5827879309654236\n",
      "0.5242168307304382\n",
      "0.5728100538253784\n",
      "0.48912206292152405\n",
      "0.45212966203689575\n",
      "0.53499436378479\n",
      "0.4958062171936035\n",
      "0.5693748593330383\n",
      "0.43712782859802246\n",
      "0.4570179879665375\n",
      "0.4939556121826172\n",
      "0.6271389126777649\n",
      "0.48762840032577515\n",
      "0.4957791268825531\n",
      "0.5595685839653015\n",
      "0.49333447217941284\n",
      "0.47492343187332153\n",
      "0.5258002877235413\n",
      "0.485037624835968\n",
      "0.4195544719696045\n",
      "0.595146656036377\n",
      "0.4547501504421234\n",
      "0.5607349872589111\n",
      "0.51602703332901\n",
      "0.5111783742904663\n",
      "0.5482367277145386\n",
      "0.46473807096481323\n",
      "0.5548348426818848\n",
      "0.4666430354118347\n",
      "0.5144751667976379\n",
      "0.5630149841308594\n",
      "0.4955999255180359\n",
      "0.5230809450149536\n",
      "0.482255756855011\n",
      "0.47485870122909546\n",
      "0.5306175351142883\n"
     ]
    }
   ],
   "source": [
    "# very crude training loop (you'll make a fancier one in your first lab)\n",
    "for epoch in range(100):\n",
    "    for x, y in iris_dl:\n",
    "        model.train()\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = lossFun(y_pred, y)\n",
    "        print(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Training\n",
    "Let's use our model to make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 1.]),\n",
       " tensor([1., 0., 0., 0., 1., 1., 0., 1., 0., 1.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(iris_dl))\n",
    "\n",
    "# some layers will do different things during training/prediction (i.e. dropout)\n",
    "model.eval()\n",
    "\n",
    "# don't compute gradients\n",
    "with torch.no_grad():\n",
    "    outputs = torch.sigmoid(model(x))\n",
    "\n",
    "y_pred = torch.zeros(10)\n",
    "y_pred[outputs > .5] = 1\n",
    "\n",
    "y_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model parameters and optimizater checkpoint\n",
    "checkpoint = {'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict' :optimizer.state_dict()}\n",
    "torch.save(checkpoint, 'model_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load them up!\n",
    "checkpoint = torch.load('model_checkpoint.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save other things in the checkpoint such as the loss history, epoch number, etc. if you really want to save every aspect of your progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip: Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class some_loss(nn.Module):\n",
    "    def __init__(self, hyperparam):\n",
    "        super(some_loss, self).__init__()\n",
    "        self.hyperparam = hyperparam\n",
    "        \n",
    "    \n",
    "    def forward(self, y_pred, y):\n",
    "        diff = y_pred - y\n",
    "        \n",
    "        # average over each entry and batch size\n",
    "        torch.norm(diff) / torch.numel(doff)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
